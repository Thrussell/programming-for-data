{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Correlations and models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7raIDLBmCVNJ"
      },
      "source": [
        "# Describing the data relatively\n",
        "---\n",
        "\n",
        "This worksheet introduces a new library: \n",
        "\n",
        "**scipy**\n",
        "\n",
        "A library of functions for interpolating, optimising, linear regression, etc\n",
        "\n",
        "---\n",
        "\n",
        "A dataset will be read into a dataframe, cleaned, wrangled and so prepared for summarising and the telling of stories through visualisations and summaries.\n",
        "\n",
        "To determine the most appropriate summaries and uses of the data, we can investigate the data to discover a range of statistical measures over an individual series or a combination of series.  A Linear regression is one such investigation.\n",
        "\n",
        "Linear regression reports correlation between the values in two series. We will be doing a simplified version of the linear regression we completed using R.\n",
        "\n",
        "To get the results of a linear regression:  \n",
        "*  import the `linregress` function from the `scipy.stats` package \n",
        "*  run the `linregress` function with the two axes assigning the result to a variable (e.g. `regression`) \n",
        "\n",
        "The result of the linregress function is a set of variables:  \n",
        "*  `slope`, `intercept` - the gradient and y-intercept of the regression line (the line of best fit between the two series) are in `regression.slope` and `regression.intercept`\n",
        "*  `rvalue` - also known as r^2 - the correlation coefficient - this indicates how closely the line drawn from the linear regression data fits the actual data, the closer to 1 the better the fit\n",
        "*  `pvalue` - the probability that you can reject the null hypothesis (that the 2 variables dont affect each other) and prove statistical significance.   \n",
        "*  the standard error is in `regression.stderr` and is the average distance of each data point from the line of best fit \n",
        "\n",
        "\n",
        "Further information [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVgw-x5VCVNN"
      },
      "source": [
        "### Exercise 1 - Clean the data \n",
        "---\n",
        "Using the positive psychology dataset - \"https://github.com/lilaceri/Working-with-data-/blob/b157a2feceb7709cf82426932385706d65446270/Data%20Sets%20for%20code%20divisio/Positive_Psychology_2017.csv?raw=true\"\n",
        "* Read the data and display info \n",
        "* Visually check the summary to see which columns have null values\n",
        "* Remove columns with a significant number of null values\n",
        "\n",
        "**Expected Output**\n",
        "```\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RangeIndex: 44 entries, 0 to 43\n",
        "Data columns (total 13 columns):\n",
        " #   Column       Non-Null Count  Dtype  \n",
        "---  ------       --------------  -----  \n",
        " 0   Age          44 non-null     int64  \n",
        " 1   English      44 non-null     object \n",
        " 2   sex          44 non-null     object \n",
        " 3   origin       44 non-null     object \n",
        " 4   Ukresidence  44 non-null     float64\n",
        " 5   MAAS         44 non-null     float64\n",
        " 6   Resilliance  44 non-null     int64  \n",
        " 7   Wellbeing    44 non-null     int64  \n",
        " 8   Stress       44 non-null     int64  \n",
        " 9   selfesteem   44 non-null     int64  \n",
        " 10  LoC          44 non-null     int64  \n",
        " 11  sleep        44 non-null     int64  \n",
        " 12  Year         44 non-null     int64  \n",
        "dtypes: float64(2), int64(8), object(3)\n",
        "memory usage: 4.6+ KB\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U5CsP0ECVNO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5_G1iWYCVNP"
      },
      "source": [
        "### Exercise 2 - Check for outliers \n",
        "---\n",
        "We briefly mentioned outliers in the R and stats presentation. An outlier is an abnormal value in the data that is either extremely high or low compared to the rest of the data. Outliers can skew results. \n",
        "\n",
        "A value is considered to be an outlier if it is > Q3 + 1.5*IQR or < Q1-1.5*IQR or more than 2 standard deviations from the mean. Q3= value at 75% of data, Q1 = value at 25% of the data, IQR is the interquartile range, the difference between Q3 and Q1.  \n",
        "\n",
        "We can check for outliers using a Box plot.\n",
        "\n",
        "*(The upper line on the boxplots arm is Q3+1.5*iqr, the lower line is Q1-1.5*iqr, the middle line of the box is the mean and the top of the box is Q3 and the bottom of the box is Q1)\n",
        "\n",
        "1. Using either matplotlib or seaborn, create a boxplot of `Wellbeing` and a separate boxplot of `selfesteem`\n",
        "2. use `plt.show()` to separate the graphs \n",
        "3. Can you see any outliers? Are they high or low?\n",
        "\n",
        "**Expected Output**\n",
        "\n",
        "https://docs.google.com/presentation/d/e/2PACX-1vQBji5MrvtdeXCtP2PJzhPLKqXXuLMYjy4nCIzXpJLoye38IzetN5amZd6pU9e4io3bTUvE6Slg_hIk/pub?start=false&loop=false&delayms=3000\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLT4pozqCVNQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VCT8LnOCVNQ"
      },
      "source": [
        "### Exercise 3 - Removing outliers\n",
        "---\n",
        "Create a function called `remove_outliers(df, column)` to remove outliers from a given column in a given dataframe.  Use the function to remove outliers from the `Wellbeing` column.  Then use the function to remove the outliers from the `selfesteem` columns. \n",
        "\n",
        "The function will:\n",
        "\n",
        "1. Store the values for Q1 and Q3 in 2 separate variables   *Hint: you can use `.quantile(0.75)` and `.quantile(0.25)` to get Q3 and Q1 respectively*  \n",
        "2. Calculate the interquartile range(IQR) using `Q3` - `Q1`  \n",
        "3. Create a variable which will store the value for the `upper_limit` (`Q3` + 1.5*`IQR`)  \n",
        "4. Create another variable which assigns the value for the `lower_limit` (`Q1` - 1.5*`IQR`)  \n",
        "5. Filter all rows where values are NOT outliers into a new dataframe called `df_normal`\n",
        "6. Return df_normal\n",
        "\n",
        "Run the function twice, once for each column.\n",
        "Show the info for the resulting dataframe\n",
        "\n",
        "**Test output**:  \n",
        "```\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Int64Index: 41 entries, 0 to 43\n",
        "Data columns (total 13 columns):\n",
        " #   Column       Non-Null Count  Dtype  \n",
        "---  ------       --------------  -----  \n",
        " 0   Age          41 non-null     int64  \n",
        " 1   English      41 non-null     object \n",
        " 2   sex          41 non-null     object \n",
        " 3   origin       41 non-null     object \n",
        " 4   Ukresidence  41 non-null     float64\n",
        " 5   MAAS         41 non-null     float64\n",
        " 6   Resilliance  41 non-null     int64  \n",
        " 7   Wellbeing    41 non-null     int64  \n",
        " 8   Stress       41 non-null     int64  \n",
        " 9   selfesteem   41 non-null     int64  \n",
        " 10  LoC          41 non-null     int64  \n",
        " 11  sleep        41 non-null     int64  \n",
        " 12  Year         41 non-null     int64  \n",
        "dtypes: float64(2), int64(8), object(3)\n",
        "memory usage: 4.5+ KB\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFvT_TtjCVNR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRs6sfE-CVNR"
      },
      "source": [
        "### Exercise 4 - Checking for normality\n",
        "---\n",
        "\n",
        "Linear regressions assume that the data is normally distributed (Gaussian) and therefore has the bell curved shape and a similar mean and median (this would be true if the mean was centred like it is in Gaussian data). \n",
        "\n",
        "1. Use seaborn's distplot to check the shape of the wellbeing and selfesteem columns.  Do they look normally distributed (Gaussian)?\n",
        "\n",
        "2. Compare the mean and median of `Wellbeing` - are they similar? \n",
        "3. Compare the mean and median of `stress` - are they similar?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOxGYfg-CVNS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nIUQF5TCVNT"
      },
      "source": [
        "### Exercise 5 - Check which variables are most highly correlated\n",
        "---\n",
        "Using the dataframe.corr() function and seaborn's heatmap, create a correlation heatmap matrix to check which variables are most highly correlated. \n",
        "  \n",
        "Values closest to 1 mean the variables are positively correlated with each other with 1 meaning 100% correlated\n",
        "Values close to -1 mean the variables are negatively correlated with each other with -1 meaning 100% negatively correlated\n",
        "\n",
        "1. create the correlation matrix assigning it to a variable using the .corr() function and rounding to 2 decimal places\n",
        "2. create a heatmap of the correlation matrix using `sns.heatmap`\n",
        "3. Which variables are the most highly correlated with each other (closest to 1 or -1)?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyYYASZvCVNT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZgZHqbHCVNU"
      },
      "source": [
        "### Exercise 6 - Linear regression \n",
        "---\n",
        "Using the `linregress` function, perform a linear regression on the Wellbeing and selfesteem columns   \n",
        "To do this:\n",
        "\n",
        "1. `from scipy.stats import linregress`\n",
        "2. create a variable 'x' which stores the Wellbeing column\n",
        "3. create a variable 'y' which stores the selfesteem column\n",
        "4. create a variable called regression and assign the result of running the linregress function with x, y as its parameters ( linregress(x,y) )\n",
        "5. display the regression\n",
        "6. display the slope (regression.slope)\n",
        "7. display the y-intercept\n",
        "8. display the r^2 value (rvalue**2)\n",
        "9. display the pvalue \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SpVEyHZCVNU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ordhi_WUCVNU"
      },
      "source": [
        "### Exercise 7 - understanding the output \n",
        "---\n",
        "The r^2 value (rvalue**2) helps us understand how much our independent variable (x) is predicting our dependent variable (y). The closer to 1 the rvalue is the more  the change in Y is explained by X. So an rvalue of 0.7 means that 70% of Y's variance can be explained by X. \n",
        "\n",
        "Our pvalue shows how significant our model is, if the pvalue is < 0.05 then the model is significant. \n",
        "\n",
        "On this basis, write below the findings of the above regression.\n",
        "* is the model significant?\n",
        "* how much is the change in y explained by x? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVCxB73eCVNV"
      },
      "source": [
        "**Write about the findings here**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7TcYPrOCVNV"
      },
      "source": [
        "### Exercise 8 - Linear regression with other variables \n",
        "---\n",
        "\n",
        "Referring back to the correlation heatmap..  \n",
        "Repeat exercise 6 but with the variables that were the most highly correlated according to the heatmap.   \n",
        "* write a comment comparing the results of this regression with the one you created in exercise 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHJi2r8zCVNV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaFhxdjEzKNL"
      },
      "source": [
        "**Comment here**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvvgY68hCVNW"
      },
      "source": [
        "### Exercise 9 - Plotting a line of best fit \n",
        "---\n",
        "To create a line of best fit we use y = slope\\*x + intercept. \n",
        "\n",
        "Using matplotlib (dont forget to import it):\n",
        "\n",
        "* create a scatter graph between Wellbeing and selfesteem \n",
        "* plot a line of best fit using the results in exercise 6  (y = slope * x + intercept)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVT8QqYCCVNW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KcUIkXHCVNW"
      },
      "source": [
        "### Exercise 10 - using seaborn to plot a regression line \n",
        "---\n",
        "\n",
        "Use seaborns regplot function to create a scatter graph with line of best fit of the variables you used in exercise 8.\n",
        "\n",
        "* Compare the 'Wellbeing' and 'selfesteem' graph to the graph you created in exercise 9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xVsVD4GCVNX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}